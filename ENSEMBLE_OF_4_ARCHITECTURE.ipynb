{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ENSEMBLE OF 4 ARCHITECTURE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0mRmjYjKGdg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5e5fcc6-a8fd-4f3c-ab10-fe4919c4bced"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k55GYgubLKUt"
      },
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.applications.densenet import DenseNet121\n",
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras import backend as K\n",
        "K.clear_session()\n",
        "import itertools\n",
        "from tensorflow.keras import layers, Model\n",
        "from keras.layers import Input, Lambda, Dense, Flatten\n",
        "from keras.models import Model\n",
        "from keras.applications.densenet import preprocess_input\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "\n",
        "from tensorflow.keras.applications.efficientnet import EfficientNetB0, preprocess_input, decode_predictions\n",
        "\n",
        "from keras.applications.densenet import DenseNet121"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHKqlknYLO2b"
      },
      "source": [
        "image_directory = '/content/gdrive/MyDrive/our_data/'\n",
        "SIZE = 224\n",
        "dataset = []   \n",
        "label = []  \n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kspbRmklLdOO"
      },
      "source": [
        "fire_images = os.listdir(image_directory + 'fire/')\n",
        "for i, image_name in enumerate(fire_images):   \n",
        "    \n",
        "    if (image_name.split('.')[1] == 'jpg'):\n",
        "        image = cv2.imread(image_directory + 'fire/' + image_name)\n",
        "        image = Image.fromarray(image, 'RGB')\n",
        "        image = image.resize((SIZE, SIZE))\n",
        "        dataset.append(np.array(image))\n",
        "        label.append(1)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiFLGE7zLmNY"
      },
      "source": [
        "non_fire_images = os.listdir(image_directory + 'non_fire/')\n",
        "for i, image_name in enumerate(non_fire_images):\n",
        "    if (image_name.split('.')[1] == 'jpg'):\n",
        "        image = cv2.imread(image_directory + 'non_fire/' + image_name)\n",
        "        image = Image.fromarray(image, 'RGB')\n",
        "        image = image.resize((SIZE, SIZE))\n",
        "        dataset.append(np.array(image))\n",
        "        label.append(0)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Utk9EBltLnaz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f0e5acb-fb51-498c-b152-12ec2ab7b131"
      },
      "source": [
        "dataset = np.array(dataset)\n",
        "label = np.array(label)\n",
        "print(len(dataset))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1628\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "355UELtJLtZ_"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#from keras.utils import to_categorical\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset, label, test_size = 0.20, random_state = 0)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7KSSR5bL1oy"
      },
      "source": [
        "from tensorflow.keras.utils import normalize\n",
        "X_train = normalize(X_train, axis=1)\n",
        "X_test = normalize(X_test, axis=1)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZBWroTRL5Mv"
      },
      "source": [
        "IMAGE_SIZE = [SIZE, SIZE]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Gw3x3TCMBXb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16b62c68-8beb-49c9-de9f-81299f58c68c"
      },
      "source": [
        "##VGG model      \n",
        "vgg = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n",
        "\n",
        "for layer in vgg.layers:\n",
        "       layer.trainable = False\n",
        "\n",
        "folders = glob('/content/gdrive/MyDrive/our_data/*')\n",
        "\n",
        "# our layers - you can add more if you want\n",
        "x = Flatten()(vgg.output)\n",
        "# x = Dense(1000, activation='relu')(x)\n",
        "prediction = Dense(len(folders), activation='sigmoid')(x) # softmax\n",
        "\n",
        "        # create a model object\n",
        "model_vgg = Model(inputs=vgg.input, outputs=prediction)\n",
        "\n",
        "        #model_vgg.summary()\n",
        "\n",
        "model_vgg.compile(optimizer='adam',  # Good default optimizer to start with\n",
        "                    loss='sparse_categorical_crossentropy',  # how will we calculate our \"error.\" Neural network aims to minimize loss.\n",
        "                      metrics=['accuracy'])  # what to track\n",
        "\n",
        "history_vgg = model_vgg.fit(X_train, y_train, epochs=1)  # train the model\n",
        "\n",
        "val_loss, val_acc = model_vgg.evaluate(X_test, y_test)  # evaluate the out of sample data with model\n",
        "\n",
        "print(\"model loss is: \",val_loss)  # model's loss (error)\n",
        "\n",
        "print(\" Model accuracy is: \", val_acc)  # model's accuracy"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41/41 [==============================] - 475s 12s/step - loss: 0.6804 - accuracy: 0.7028\n",
            "11/11 [==============================] - 121s 11s/step - loss: 0.5704 - accuracy: 0.7761\n",
            "model loss is:  0.5704332590103149\n",
            " Model accuracy is:  0.7760736346244812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySgZ2Q8hMEWn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2f6fbea-9be3-44a9-e6f0-83115c2bc8c8"
      },
      "source": [
        "#RESNET50\n",
        "resnet = ResNet50(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n",
        "\n",
        "\n",
        "for layer in resnet.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "folders = glob('/content/gdrive/MyDrive/our_data/*')\n",
        "\n",
        "\n",
        "x = Flatten()(resnet.output)\n",
        "# x = Dense(1000, activation='relu')(x)\n",
        "prediction = Dense(len(folders), activation='softmax')(x)\n",
        "\n",
        "\n",
        "model_ResNet = Model(inputs=resnet.input, outputs=prediction)\n",
        "\n",
        "# model.summary()\n",
        "\n",
        "model_ResNet.compile(optimizer='adam',  # Good default optimizer to start with\n",
        "             loss='sparse_categorical_crossentropy',  # how will we calculate our \"error.\" Neural network aims to minimize loss.\n",
        "              metrics=['accuracy'])  # what to track\n",
        "\n",
        "history_ResNet= model_ResNet.fit(X_train, y_train, epochs=1)  # train the model\n",
        "\n",
        "val_loss, val_acc = model_ResNet.evaluate(X_test, y_test)  # evaluate the out of sample data with model\n",
        "\n",
        "print(\"model loss is: \",val_loss)  # model's loss (error)\n",
        "\n",
        "print(\" Model accuracy is: \", val_acc)  # model's accuracy"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n",
            "94781440/94765736 [==============================] - 1s 0us/step\n",
            "41/41 [==============================] - 144s 3s/step - loss: 1.7957 - accuracy: 0.5975\n",
            "11/11 [==============================] - 36s 3s/step - loss: 1.6676 - accuracy: 0.7515\n",
            "model loss is:  1.667637586593628\n",
            " Model accuracy is:  0.7515337467193604\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWm-zUpSMKJ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca643719-1cf3-4284-99b2-86c6d9cbe8bc"
      },
      "source": [
        "efficientnet = EfficientNetB0(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n",
        "\n",
        "for layer in efficientnet.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "  folders = glob('/content/gdrive/MyDrive/our_data/*')\n",
        "\n",
        "  x = Flatten()(efficientnet.output)\n",
        "# x = Dense(1000, activation='relu')(x)\n",
        "prediction = Dense(len(folders), activation='softmax')(x)\n",
        "\n",
        "\n",
        "model_Efficient = Model(inputs=efficientnet.input, outputs=prediction)\n",
        "\n",
        "\n",
        "#model_Efficient.summary()\n",
        "\n",
        "model_Efficient.compile(optimizer='adam',  # Good default optimizer to start with\n",
        "             loss='sparse_categorical_crossentropy',  # how will we calculate our \"error.\" Neural network aims to minimize loss.\n",
        "              metrics=['accuracy'])  # what to track\n",
        "\n",
        "history_Efficient= model_ResNet.fit(X_train, y_train, epochs=1)  # train the model\n",
        "\n",
        "val_loss, val_acc = model_Efficient.evaluate(X_test, y_test)  # evaluate the out of sample data with model\n",
        "\n",
        "print(\"model loss is: \",val_loss)  # model's loss (error)\n",
        "\n",
        "print(\" Model accuracy is: \", val_acc)  # model's accuracy"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "16711680/16705208 [==============================] - 0s 0us/step\n",
            "16719872/16705208 [==============================] - 0s 0us/step\n",
            "41/41 [==============================] - 141s 3s/step - loss: 1.1431 - accuracy: 0.6152\n",
            "11/11 [==============================] - 13s 970ms/step - loss: 0.5908 - accuracy: 0.7515\n",
            "model loss is:  0.5907837748527527\n",
            " Model accuracy is:  0.7515337467193604\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBW5IJdKMPuL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27dcb011-08b4-42e8-8f23-02c5e572c72d"
      },
      "source": [
        "#DENSENET\n",
        "densenet = DenseNet121(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n",
        "\n",
        "for layer in densenet.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "folders = glob('/content/gdrive/MyDrive/our_data/*')\n",
        "\n",
        "x = Flatten()(densenet.output)\n",
        "# x = Dense(1000, activation='relu')(x)\n",
        "prediction = Dense(len(folders), activation='softmax')(x)\n",
        "\n",
        "model_DenseNet = Model(inputs=densenet.input, outputs=prediction)\n",
        "\n",
        "#model_DenseNet.summary()\n",
        "\n",
        "model_DenseNet.compile(optimizer='adam',  # Good default optimizer to start with\n",
        "              loss='sparse_categorical_crossentropy',# how will we calculate our \"error.\" Neural network aims to minimize loss.\n",
        "              metrics=['accuracy'])  # what to track\n",
        "             \n",
        "history_DenseNet = model_DenseNet.fit(X_train, y_train, epochs=1) \n",
        "\n",
        "val_loss, val_acc = model_DenseNet.evaluate(X_test, y_test)  # evaluate the out of sample data with model\n",
        "print(\"model loss is: \",val_loss)  # model's loss (error)\n",
        "\n",
        "print(\" Model accuracy is: \", val_acc)  # model's accuracy"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "29089792/29084464 [==============================] - 1s 0us/step\n",
            "29097984/29084464 [==============================] - 1s 0us/step\n",
            "41/41 [==============================] - 141s 3s/step - loss: 1.2138 - accuracy: 0.7880\n",
            "11/11 [==============================] - 35s 3s/step - loss: 0.3835 - accuracy: 0.8558\n",
            "model loss is:  0.3834618628025055\n",
            " Model accuracy is:  0.8558282256126404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZL04z8XahKT"
      },
      "source": [
        "pred1=model_vgg.predict(X_test)\n",
        "pred2=model_ResNet.predict(X_test)\n",
        "pred3=model_Efficient.predict(X_test)\n",
        "pred4=model_DenseNet.predict(X_test)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kauBN6wlMVF2"
      },
      "source": [
        "#Ensemble max voting\n",
        "\n",
        "import math\n",
        "import sys\n",
        "#final_pred = np.array([])\n",
        "final_pred = []\n",
        "#print(len(X_test))\n",
        "#sys.exit()\n",
        "\n",
        "for i in range(0,len(X_test)):\n",
        "    temp = []\n",
        "    temp.append(np.argmax(pred1[i]))\n",
        "    temp.append(np.argmax(pred2[i]))\n",
        "    temp.append(np.argmax(pred3[i]))\n",
        "    temp.append(np.argmax(pred4[i]))\n",
        "\n",
        "    one= sum(temp)\n",
        "    zero= 4 - one\n",
        "    \n",
        "    \n",
        "   \n",
        "    if one>=zero:\n",
        "      final_pred.append(1)\n",
        "    else:\n",
        "      final_pred.append(0)\n",
        "\n",
        "final_pred = np.array(final_pred)\n"
      ],
      "execution_count": 29,
      "outputs": []
    }
  ]
}